{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DdlMv-eIoWxJssVyPqzJ2gEPAEpPrOuC",
      "authorship_tag": "ABX9TyMazcu+Ev6jXcSIXqz8ABen",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khemsu/NLP_Training-models/blob/main/GradientBoostingClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas nltk scikit-learn rouge-score transformers\n",
        "!python -m nltk.downloader punkt stopwords averaged_perceptron_tagger"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4wB29DeccR",
        "outputId": "3c54b5d2-6f62-4d34-f242-e1e607cad403"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7df61891b151ff53673b1740128df294fb8158daa62e8e8654796dbf9928dc12\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from rouge_score import rouge_scorer\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "MIXMygzwfyUG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths to your BBC dataset\n",
        "articles_path = \"/content/drive/MyDrive/BBC News Summary/News Articles/business\"\n",
        "summaries_path = \"/content/drive/MyDrive/BBC News Summary/Summaries/business\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iXm6nGAf0RX",
        "outputId": "45de7fc7-1e19-4f7a-ac6e-9e2f6ec4fb4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_files(filepath_list, folder_path):\n",
        "    \"\"\"Read all .txt files and return their contents with proper encoding\"\"\"\n",
        "    contents = []\n",
        "    for filename in filepath_list:\n",
        "        with open(os.path.join(folder_path, filename), 'r', encoding='latin1') as f:\n",
        "            contents.append(f.read())\n",
        "    return contents\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Clean and tokenize text while preserving sentence structure\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    clean_sentences = []\n",
        "    for sent in sentences:\n",
        "        # Basic cleaning\n",
        "        words = word_tokenize(sent.lower())\n",
        "        words = [w for w in words if w.isalnum() and w not in stopwords.words('english')]\n",
        "        clean_sentences.append(' '.join(words))\n",
        "    return clean_sentences\n"
      ],
      "metadata": {
        "id": "A380069tf3gV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and pair data\n",
        "article_files = sorted(os.listdir(articles_path))\n",
        "summary_files = sorted(os.listdir(summaries_path))\n",
        "\n",
        "articles = read_text_files(article_files, articles_path)\n",
        "summaries = read_text_files(summary_files, summaries_path)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'article': articles,\n",
        "    'summary': summaries,\n",
        "    'article_filename': article_files,\n",
        "    'summary_filename': summary_files\n",
        "})"
      ],
      "metadata": {
        "id": "_qN94WQ9mO_4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loaded {len(df)} article-summary pairs\")\n",
        "print(\"\\nSample pair verification:\")\n",
        "print(\"Article:\", df['article'][0][:100] + \"...\")\n",
        "print(\"Summary:\", df['summary'][0][:100] + \"...\")"
      ],
      "metadata": {
        "id": "Vc3MdzRbmRjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b41922-8535-4ee8-9252-22d3ab799f98"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 510 article-summary pairs\n",
            "\n",
            "Sample pair verification:\n",
            "Article: Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.1...\n",
            "Summary: TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.For the full-year, TimeWarner p...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_advanced_features(sentences):\n",
        "    \"\"\"Extract multiple linguistic features for each sentence\"\"\"\n",
        "    features = []\n",
        "    positions = np.linspace(0, 1, num=len(sentences))\n",
        "    tfidf = TfidfVectorizer().fit_transform(sentences)\n",
        "    tfidf_avg = np.array(tfidf.mean(axis=1)).flatten()\n",
        "\n",
        "    for i, sent in enumerate(sentences):\n",
        "        # Quantitative features\n",
        "        numbers = len(re.findall(r'\\$?\\d+(?:\\.\\d+)?%?', sent))\n",
        "        proper_nouns = len([word for word, tag in pos_tag(word_tokenize(sent)) if tag == 'NNP'])\n",
        "\n",
        "        # Structural features\n",
        "        word_count = len(word_tokenize(sent))\n",
        "        char_count = len(sent)\n",
        "        is_question = 1 if sent.strip().endswith('?') else 0\n",
        "\n",
        "        # Semantic features (simplified)\n",
        "        has_connector = 1 if any(word in sent for word in ['however', 'therefore', 'although']) else 0\n",
        "\n",
        "        features.append([\n",
        "            positions[i],      # Normalized position (0=start, 1=end)\n",
        "            tfidf_avg[i],     # TF-IDF importance\n",
        "            numbers,          # Count of numerical values\n",
        "            proper_nouns,     # Count of proper nouns\n",
        "            word_count,       # Word length\n",
        "            char_count/100,   # Normalized character length\n",
        "            is_question,      # Is question sentence\n",
        "            has_connector     # Contains discourse connector\n",
        "        ])\n",
        "\n",
        "    return np.array(features)"
      ],
      "metadata": {
        "id": "r3Esco9OmXfE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_sentences(articles, summaries):\n",
        "    \"\"\"Generate binary labels (1=in summary, 0=not in summary)\"\"\"\n",
        "    y_labels = []\n",
        "    for art_sents, sum_sents in zip(articles, summaries):\n",
        "        labels = []\n",
        "        for art_sent in art_sents:\n",
        "            # Check if any summary sentence is contained in article sentence\n",
        "            match = 0\n",
        "            for sum_sent in sum_sents:\n",
        "                if sum_sent in art_sent or art_sent in sum_sent:\n",
        "                    match = 1\n",
        "                    break\n",
        "            labels.append(match)\n",
        "        y_labels.append(labels)\n",
        "    return y_labels"
      ],
      "metadata": {
        "id": "_Q2SYwBkmaJr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download all required NLTK data\n",
        "nltk.download([\n",
        "    'punkt',          # Tokenizer\n",
        "    'stopwords',      # Stopwords list\n",
        "    'wordnet',        # WordNet lemmatizer\n",
        "    'averaged_perceptron_tagger_eng'  # POS tagger\n",
        "])\n",
        "\n",
        "\n",
        "# Preprocess and label\n",
        "df['clean_article'] = df['article'].apply(preprocess)\n",
        "df['clean_summary'] = df['summary'].apply(preprocess)\n",
        "y_labels = label_sentences(df['clean_article'], df['clean_summary'])\n",
        "\n",
        "## 4. Model Training =====================================================\n",
        "\n",
        "# Feature extraction\n",
        "X_features = []\n",
        "for sentences in df['clean_article']:\n",
        "    features = extract_advanced_features(sentences)\n",
        "    X_features.append(features)\n",
        "\n",
        "# Prepare training data\n",
        "X = np.vstack(X_features)\n",
        "y = np.concatenate(y_labels)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train calibrated model\n",
        "model = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "calibrated_model = CalibratedClassifierCV(model, cv=5)\n",
        "calibrated_model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {calibrated_model.score(X_train, y_train):.2f}\")\n",
        "print(f\"Test Accuracy: {calibrated_model.score(X_test, y_test):.2f}\")"
      ],
      "metadata": {
        "id": "maqFV-jcmeJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8144016-bac2-4234-c276-d407b121ec42"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Accuracy: 0.92\n",
            "Test Accuracy: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(article, model, top_n=3):\n",
        "    \"\"\"Generate extractive summary with diversity and coherence\"\"\"\n",
        "    sentences = sent_tokenize(article)\n",
        "    if len(sentences) <= top_n:\n",
        "        return article\n",
        "\n",
        "    clean_sents = preprocess(article)\n",
        "    features = extract_advanced_features(clean_sents)\n",
        "    probas = model.predict_proba(features)[:, 1]\n",
        "\n",
        "    # Select top sentences with diversity\n",
        "    selected_indices = []\n",
        "    for _ in range(top_n):\n",
        "        remaining = [i for i in range(len(sentences)) if i not in selected_indices]\n",
        "        next_idx = remaining[np.argmax(probas[remaining])]\n",
        "        selected_indices.append(next_idx)\n",
        "\n",
        "    # Maintain original order\n",
        "    selected_indices.sort()\n",
        "    summary = ' '.join([sentences[i] for i in selected_indices])\n",
        "\n",
        "    # Post-processing\n",
        "    summary = summary[0].upper() + summary[1:]\n",
        "    if not summary.endswith(('.', '!', '?')):\n",
        "        summary = summary + '.'\n",
        "    return summary\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate on sample\n",
        "sample_idx = 0\n",
        "generated = generate_summary(df['article'][sample_idx], calibrated_model)\n",
        "reference = df['summary'][sample_idx]\n",
        "\n",
        "print(\"\\n=== Original Article ===\")\n",
        "print(df['article'][sample_idx][:500] + \"...\")\n",
        "\n",
        "print(\"\\n=== Reference Summary ===\")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\n=== Generated Summary ===\")\n",
        "print(generated)\n",
        "\n",
        "# ROUGE Evaluation\n",
        "scores = scorer.score(reference, generated)\n",
        "print(\"\\n=== ROUGE Scores ===\")\n",
        "print(f\"ROUGE-1: F1={scores['rouge1'].fmeasure:.3f} (Recall={scores['rouge1'].recall:.3f}, Precision={scores['rouge1'].precision:.3f})\")\n",
        "print(f\"ROUGE-2: F1={scores['rouge2'].fmeasure:.3f} (Recall={scores['rouge2'].recall:.3f}, Precision={scores['rouge2'].precision:.3f})\")\n",
        "print(f\"ROUGE-L: F1={scores['rougeL'].fmeasure:.3f} (Recall={scores['rougeL'].recall:.3f}, Precision={scores['rougeL'].precision:.3f})\")"
      ],
      "metadata": {
        "id": "J6B14QdsmhIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ea56dc-a656-4b01-c38a-e7f45216e094"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Original Article ===\n",
            "Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n",
            "\n",
            "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
            "\n",
            "Time ...\n",
            "\n",
            "=== Reference Summary ===\n",
            "TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn.Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters.Time Warner's fourth quarter profits were slightly better than analysts' expectations.\n",
            "\n",
            "=== Generated Summary ===\n",
            "TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n",
            "\n",
            "=== ROUGE Scores ===\n",
            "ROUGE-1: F1=0.550 (Recall=0.379, Precision=1.000)\n",
            "ROUGE-2: F1=0.536 (Recall=0.368, Precision=0.982)\n",
            "ROUGE-L: F1=0.550 (Recall=0.379, Precision=1.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ekwLikVPHEwF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}